\documentclass[10pt]{article}
\usepackage{fullpage,amsthm,amsmath,amssymb,enumitem,url,verbatim,graphicx}
\usepackage{color}
\usepackage{biblatex}
\usepackage{titlesec}

\addbibresource{references.bib}

\begin{document}

\title{\Large CS 267 Project Proposal: Searching for and Clustering \\
Strings that Satisfy Regular Expressions in Real Data}
\author{\large Andrew Head}
\date{}
\maketitle

% Section resizing hint from
% http://tex.stackexchange.com/questions/103286/how-to-change-section-subsection-font-size
\titleformat{\section}{\normalfont\fontsize{11}{13}\bfseries}{\thesection}{1em}{}

\vspace{-5ex}
\section{Problem}

Programmers search for help online when they're programming  to learn new technologies and clarify details~\cite{brandt_two_2009}.
Though sometimes it can be difficult to use online examples, as authors may assume that readers have certain knowledge they don't.
In past research, I built tools to automatically explain micro-languages like CSS selectors and regular expressions when they appear in web pages~\cite{head_tutorons_2015}.

I generated strings matching arbitrary regular expressions to help show what the regular expressions were designed to do.
But they were hard to read and unrepresentative of the text it was probably built to match.
I want to make it tractable to find and select representative strings that a regular expression matches across many real datasets.
\textbf{Given an arbitrary regular expression that a user discovers, I will produce
three real, representative strings the regular expression matches in interactive time.}

\section{Proposal}

To find real, representative strings that a regular expression matches, I propose these steps:
\begin{enumerate}[noitemsep]
\item Collect a few hundred/thousand datasets with heterogeneous data
\item Parallelize a regular expression matching routine across machines to the extent where it discovers a subset of matching strings covering all datasets in sub-second time
\item Clustering resulting strings based on matching rules satisfied
\end{enumerate}
My contribution related to this class will be adapting existing regular expression matching routines to a parallel architecture, or parallelizing string clustering for this application.
If I find performance of ``embarrassingly parallel'' string search is good enough with existing tools (e.g., \texttt{egrep}), I will focus on clustering.

Performance will be evaluated with 100 POSIX-style extended regular expressions that I have found in programming tutorial corpus I extracted for a past project.
It will be measured as the runtime for scanning a subset of each of the subsets and reporting all valid strings, and as the time to cluster the resulting strings.
I will perform more detailed, step-by-step analysis for a small sample of these regular expressions.

\section{Progress}

I have downloaded 1304 CSV datasets from \url{Data.gov}, and intend to download 10,000 total.
I have verified that these datasets include email addresses, phone numbers, and times, and street addresses.
I have run \texttt{egrep} on a local server over all files smaller than 512MB, and seen that it takes \emph{lots of time} to complete, giving me a baseline and confirming that a faster solution is needed.

\printbibliography{}

\end{document}
