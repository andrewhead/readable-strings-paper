\documentclass[10pt]{article}
\usepackage{fullpage,amsthm,amsmath,amssymb,enumitem,url,verbatim,graphicx}
\usepackage{color}
\usepackage{biblatex}
\usepackage{titlesec}

\addbibresource{references.bib}

\begin{document}

\title{\Large CS 267 Project Proposal: Searching for and Clustering \\
Strings from Real Data that Satisfy Regular Expressions}
\author{\large Andrew Head}
\date{}
\maketitle

% Section resizing hint from
% http://tex.stackexchange.com/questions/103286/how-to-change-section-subsection-font-size
\titleformat{\section}{\normalfont\fontsize{10}{11}\bfseries}{\thesection}{1em}{}

\pagenumbering{gobble}

\vspace{-5ex}
\section{Problem}

Programmers search for help online when they're programming to learn new technologies and clarify details~\cite{brandt_two_2009}.
Sometimes it can be difficult to use online examples, as authors may assume that readers have certain knowledge they don't.
In past research, I built tools to automatically explain micro-languages like CSS selectors and regular expressions when they appear in web tutorials.%~\cite{head_tutorons_2015}.

I generated strings matching arbitrary regular expressions to help show what the regular expressions were designed to do.
But they were hard to read and unrepresentative of the text they were probably built to match.
I want to make it tractable to select representative strings that a regular expression matches across many real datasets.
\textbf{Given an arbitrary regular expression a user discovers, my goal is to produce
a few real, representative strings it matches in interactive time, sampled from real datasets.}

\section{Proposal}

To find real, representative strings that a regular expression matches, I propose these steps:
\begin{enumerate}[noitemsep]
\item Collect a few hundred/thousand datasets with heterogeneous data
\item Parallelize a regular expression matching routine across machines to the extent where it discovers a subset of matching strings covering all datasets in sub-second time
\item Cluster matching strings based on paths followed in the regular expression DFAs
\end{enumerate}
My contribution will be adapting existing regular expression matching routines to a parallel architecture, or parallelizing string clustering for this application.
If I find performance of ``embarrassingly parallel'' string search over subsets of datasets is good enough with using existing tools (e.g., \texttt{egrep}), I will focus on adapting parallel clustering to this problem.
Techniques like~\cite{zhao_parallel_2009} may provide baseline clustering implementations.

Performance will be evaluated with 100 POSIX-style extended regular expressions that I have found in a programming tutorial corpus I extracted for a past project.
I will measure performance for scanning a subset of each of the datasets and reporting all valid strings, and the time to cluster the resulting strings.
I will perform more detailed, step-by-step analysis for a small sample of these regular expressions.

\section{Progress}

I have downloaded 2026 CSV datasets from \url{Data.gov}, and intend to download 10,000 total.
I have verified that these datasets include email addresses, phone numbers, and times, and street addresses.
I have run \texttt{egrep} for a five-character regular expression for time on a local server with over 13GB of data as a baseline.
This took 30.42 seconds of CPU time to complete, confirming that a faster solution is needed for interactivity.

\printbibliography{}

\end{document}
